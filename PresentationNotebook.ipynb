{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoEdstem Presentation Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup functions\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Document\n",
    "from IPython.display import Markdown, display\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "with open(\"./secret_api_key\") as f:\n",
    "  secret_key = f.read()\n",
    "  openai.api_key = secret_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import connections, Collection, utility\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "\n",
    "from llama_index.core import StorageContext\n",
    "def read_documents(data_path):\n",
    "  return SimpleDirectoryReader(data_path, recursive=True).load_data()\n",
    "\n",
    "def build_new_query_engine(data_path, collection_name):\n",
    "  vector_store = MilvusVectorStore(dim=1536, overwrite=True, collection_name=collection_name)\n",
    "  documents = read_documents(data_path)\n",
    "  storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "  index = VectorStoreIndex.from_documents(documents, storage_context, show_progress=True)\n",
    "  query_engine = index.as_query_engine()\n",
    "  return query_engine\n",
    "  \n",
    "\n",
    "def load_existing_index_as_query_engine(collection_name, k = 10):\n",
    "  connections.connect(\n",
    "    alias=\"default\", \n",
    "    host='localhost', \n",
    "    port='19530'\n",
    "  )\n",
    "  \n",
    "  if collection_name not in utility.list_collections():\n",
    "    print(f\"Collection {collection_name} not found\")\n",
    "    return\n",
    "  \n",
    "  Collection(collection_name).load()\n",
    "  vector_store = MilvusVectorStore(dim=1536, overwrite=False, collection_name=collection_name)\n",
    "  index = VectorStoreIndex.from_vector_store(vector_store)\n",
    "  query_engine = index.as_query_engine()\n",
    "  query_engine.retriever.similarity_top_k = k\n",
    "\n",
    "  return query_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_index(collection_name):\n",
    "  connections.connect(\n",
    "    alias=\"default\", \n",
    "    host='localhost', \n",
    "    port='19530'\n",
    "  )\n",
    "  \n",
    "  if collection_name not in utility.list_collections():\n",
    "    print(f\"Collection {collection_name} not found\")\n",
    "    return\n",
    "  \n",
    "  Collection(collection_name).load()\n",
    "  vector_store = MilvusVectorStore(dim=1536, overwrite=False, collection_name=collection_name)\n",
    "  index = VectorStoreIndex.from_vector_store(vector_store)\n",
    "  return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(x):\n",
    "  display(Markdown(str(x)))\n",
    "\n",
    "response_list = []\n",
    "\n",
    "def append_and_print(resp, response_list = response_list):\n",
    "  response_list += [resp]\n",
    "  pretty_print(resp)\n",
    "\n",
    "#Full transparency of data\n",
    "def print_data_sources(response, sources=100, show_text= True):\n",
    "  for i in range(min(len(response.source_nodes), sources)):\n",
    "      pretty_print(\"Data retrieved from file: \" + response.source_nodes[i].node.metadata.get('file_name'))\n",
    "      if show_text:\n",
    "        pretty_print(\"Data used to generate response: \" + response.source_nodes[i].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key= openai.api_key,\n",
    ")\n",
    "\n",
    "def query_gpt3(query):\n",
    "  chat_completion = openai_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": query,\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "  )\n",
    "  return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_MILVUS_COLLECION_NAME = \"final_milvus_collection\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_query_engine = load_existing_index_as_query_engine(FINAL_MILVUS_COLLECION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To run the SQL Assignment, make sure Docker and Docker Compose are installed on your system. Start the Docker daemon and verify its status by running `$ docker info`. Next, execute `$ docker-compose up` in a terminal. Once the process is stable, go to http://localhost:8888 in a browser to access the JupyterLab interface. Open the 'Assignment.ipynb' file from the sidebar to start the assignment."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sql_question = \"How do I run the SQL Assignment\"\n",
    "append_and_print(full_query_engine.query(sql_question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To run an SQL assignment, you will need access to a database management system (such as MySQL, PostgreSQL, SQL Server, etc.) and a tool to write and execute SQL queries (such as MySQL Workbench, pgAdmin, SQL Server Management Studio, etc.).\n",
       "\n",
       "Here are the steps to run an SQL assignment:\n",
       "\n",
       "1. Set up your database system: If you don't have a database system installed, you will need to download and install one. Follow the instructions provided by the database system's website to install and set up the database.\n",
       "\n",
       "2. Create a new database: Create a new database where you will be running the SQL assignment. You can do this by using the CREATE DATABASE statement.\n",
       "\n",
       "3. Connect to the database: Use the database management tool to connect to the newly created database. You will need to provide the database connection details (such as hostname, port, username, password) to establish a connection.\n",
       "\n",
       "4. Write and execute SQL queries: Write the SQL queries provided in the assignment using the database management tool's query editor. You can execute the queries by clicking on the \"Execute\" or \"Run\" button.\n",
       "\n",
       "5. Review the results: Once you run the queries, review the results to check if they meet the requirements specified in the assignment.\n",
       "\n",
       "6. Submit your assignment: If you are required to submit the results of the SQL assignment, you can do so by sending the output of the queries or a screenshot of the results to your instructor or the designated submission platform.\n",
       "\n",
       "Remember to always follow the specific instructions provided in the SQL assignment and reach out to your instructor or classmates for help if you encounter any difficulties."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pretty_print(query_gpt3(sql_question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transparency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Data retrieved from file: Assignment 2 _ SQL.txt"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Data used to generate response: ﻿SQL\r\n",
       "Due: Monday, September 25, 2023 at 10:00 pm EDT\r\n",
       "\r\n",
       "\r\n",
       "1 Introduction        1\r\n",
       "2 Background Knowledge        2\r\n",
       "2.1 SQL        2\r\n",
       "2.2 Jupyter Notebook        2\r\n",
       "2.3 Docker        3\r\n",
       "3 Assignment Spec        4\r\n",
       "3.1 Overview        4\r\n",
       "3.2 Getting Started        8\r\n",
       "4 FAQ        8\r\n",
       "Feedback        8\r\n",
       "\r\n",
       "\r\n",
       "NOTE: Please make sure you read the handout all the way through before getting started or going to hours!\r\n",
       "________________\r\n",
       "\r\n",
       "\r\n",
       "1        Introduction\r\n",
       "In this assignment, we will dive into Structured Query Language (SQL), a powerful tool for managing and manipulating relational databases. SQL is a standard programming language designed for managing data stored in databases, and it plays a crucial role in retrieving, inserting, updating, and deleting data. \r\n",
       "\r\n",
       "\r\n",
       "Throughout this assignment, you will have the opportunity to strengthen your SQL skills and apply your knowledge to real-world scenarios by working with the IMDb database. You will be presented with a series of tasks and challenges that will test your understanding of SQL concepts, especially database querying and extracting the database design. By completing this assignment, you will gain hands-on experience in writing SQL queries, understanding complex database relationships, and leveraging SQL to extract valuable insights from data. \r\n",
       "2        Background Knowledge\r\n",
       "2.1 SQL\r\n",
       "To get started with SQL, check out our Guide to SQL! Even if you're already proficient, everybody benefits from a brief refresher.\r\n",
       "\r\n",
       "\r\n",
       "Note that in this project, we are using sqlite3 - this means that not all features you learn about in class are necessarily available. We assure you, however, that all of the problems can be solved in sqlite3.\r\n",
       "2.2 Jupyter Notebook\r\n",
       "Jupyter Notebook ( formerly IPython Notebook) is an interactive web application for creating and sharing computational documents. The project was first named IPython and later renamed Jupyter in 2014. It is a fully open-source product, and users can use every functionality available for free. This notebook supports more than 40 languages, especially Python.\r\n",
       "\r\n",
       "\r\n",
       "Why do we use Jupyter Notebook? Jupyter notebooks are used for a variety of purposes. A notebook is an interactive computational environment in which users can execute a particular piece of code, observe the output, and make changes to the code to drive it to the desired output or explore more. This would be helpful during the debugging process for the SQL queries compared to using sqlite3 independently. [1]\r\n",
       "\r\n",
       "\r\n",
       "For the documentation of the Jupyter Notebook, check here\r\n",
       "________________\r\n",
       "2.3 Docker\r\n",
       "Instead of making you install the Jupyter Notebook independently for this assignment, we’ll prepare the “Docker” for you to install. But first, we will introduce you to what the docker is.\r\n",
       "\r\n",
       "\r\n",
       "Docker is an open platform for developing, shipping and running applications. In other words, Docker is a “package” in the form of containers that prepare the environment (Mostly a set of Library) for you to run the program. We decided to use this instead of installing the programs independently to decrease the complexity of the assignment and to assist students using Windows and Linux regarding the Version and the file location. \r\n",
       "\r\n",
       "\r\n",
       "3        Assignment Spec\r\n",
       "3.1 Overall\r\n",
       "In this assignment, you will write the SQL queries based on the questions we give. You can get the assignment by accepting the GitHub classroom assignment here\r\n",
       "\r\n",
       "\r\n",
       "The following are the files you will need to alter:\r\n",
       "\r\n",
       "\r\n",
       "* ./notebooks/Assignment.py\r\n",
       "\r\n",
       "\r\n",
       "Since you will only be writing queries, you should NOT alter the database. If you do so, you may need to restart your container for a fresh database file.\r\n",
       "\r\n",
       "\r\n",
       "All your queries should be inside the defined strings ex:\r\n",
       "pxqy = “””\r\n",
       "SELECT * FROM EXAMPLE \r\n",
       "“””\r\n",
       "The autograder will use these query strings. Do NOT modify variable names in the form of pxqy (ex: p1q1), or else your implementation will fail the autograder!\r\n",
       "3.2 Getting Started\r\n",
       "Ensure that you have Docker and Docker Compose installed.\r\n",
       "\r\n",
       "\r\n",
       "Start the docker daemon and ensure it’s running by running:\r\n",
       "$ docker info\r\n",
       "\r\n",
       "\r\n",
       "You should be able to run the following command in a terminal:\r\n",
       "$ docker-compose up \r\n",
       "\r\n",
       "\r\n",
       "Note that this command will run a process that will not resolve in your terminal. Once it has stabilized (i.e, it is no longer producing new logs), open http://localhost:8888 in a browser. You should see a JupyterLab interface. Open the file in the sidebar named 'Assignment.ipynb' and start the assignment.\r\n",
       "4        FAQ\r\n",
       "\r\n",
       "\r\n",
       "* How is this graded?: See the course guide. \r\n",
       "* How do I hand this in?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Data retrieved from file: Assignment 2 _ SQL.txt"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Data used to generate response: Do NOT modify variable names in the form of pxqy (ex: p1q1), or else your implementation will fail the autograder!\r\n",
       "3.2 Getting Started\r\n",
       "Ensure that you have Docker and Docker Compose installed.\r\n",
       "\r\n",
       "\r\n",
       "Start the docker daemon and ensure it’s running by running:\r\n",
       "$ docker info\r\n",
       "\r\n",
       "\r\n",
       "You should be able to run the following command in a terminal:\r\n",
       "$ docker-compose up \r\n",
       "\r\n",
       "\r\n",
       "Note that this command will run a process that will not resolve in your terminal. Once it has stabilized (i.e, it is no longer producing new logs), open http://localhost:8888 in a browser. You should see a JupyterLab interface. Open the file in the sidebar named 'Assignment.ipynb' and start the assignment.\r\n",
       "4        FAQ\r\n",
       "\r\n",
       "\r\n",
       "* How is this graded?: See the course guide. \r\n",
       "* How do I hand this in?: \r\n",
       "   * For the submission, save the assignment notebook as a Python file by going to File >> Save and Export Notebook as >> Executable Script (the file should be called \"SQL_Assignment.py\"). \r\n",
       "   * Then, submit this \"SQL_Assignment.py\" file to Gradescope under \"Assignment 2: SQL\" (using the \"Upload\" option, not submitting through GitHub).\r\n",
       "   * ENSURE YOUR SUBMITTED FILE IS NAMED “SQL_Assignment.py” YOUR SUBMISSION WILL FAIL THE AUTOGRADER OTHERWISE\r\n",
       "* What packages can I import?: You may import any standard library package that doesn’t trivialize the assignment. If you are unsure whether a package is a fair game, please post on Edstem. \r\n",
       "\r\n",
       "\r\n",
       "Feedback\r\n",
       "\r\n",
       "\r\n",
       "As this is a new course, we appreciate any feedback you have for us! If you enjoyed this assignment, hated this assignment, or have other thoughts to share, please do so here!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Data retrieved from file: Slide 2 - SQL Gearup.txt"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Data used to generate response: SQL Gearup\n",
       "CSCI 1270 - Database Management Systems\n",
       "\n",
       "Please read the handout in its entirety!\n",
       "It will save you lots of time per assignment; especially the hints!\n",
       "If anything is unclear about the handout, please post on EdStem.\n",
       "The Handout\n",
       "Common Commands in SQLITE\n",
       "Foreign Keys\n",
       "DML commands\n",
       "CASE statements\n",
       "Tconst, nconst\n",
       "Fscore\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "Highly recommend doing Guide to SQL, then checking out the supplementary information.\n",
       "In this project, we are using jupyter notebook\n",
       "If you need help getting set up, please come to TA Hours or post on EdStem!\n",
       "\n",
       "SQL\n",
       "\n",
       "An open platform for developing, shipping and running applications\\\n",
       "Why Docker?\n",
       "Decrease the complexity of the assignment \n",
       "Solve OS differences\n",
       "\n",
       "Docker\n",
       "\n",
       "Install Docker and Docker Compose \n",
       "Once these two software are installed, you should be able to run the following command in a terminal:\n",
       "$ docker-compose up \n",
       "Once it has stabilized (i.e, it is no longer producing new logs), open http://localhost:8888 in a browser. You should see a JupyterLab interface. Open the file in the sidebar named 'SQL_Assignment.ipynb' and start the assignment.\n",
       "Getting Started\n",
       "\n",
       "The Codebase\n",
       "Do not change the name of the variables since we depend on them for the grading purpose\n",
       "\n",
       "\n",
       "Part 1,2,3: SQL queries\n",
       "Run queries within the Jupyter Notebook\n",
       "The result should appear after running the code, easier for the debugging\n",
       "\n",
       "\n",
       "\n",
       "Tips\n",
       "Read the handout thoroughly!\n",
       "Be comfortable in SQL before starting."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_data_sources(response_list[4], 3, show_text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impacts of different K-Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To implement concurrent transactions for the concurrency assignment, you will need to create multiple threads or processes that can operate concurrently to execute transactions on your database. Here are the steps you can follow to implement concurrent transactions:\n",
       "\n",
       "1. Choose a programming language or framework that supports concurrency, such as Java, Python, or Node.js.\n",
       "\n",
       "2. Set up a database system that supports transactions, such as PostgreSQL, MySQL, or Oracle.\n",
       "\n",
       "3. Create a table or collection in your database to store the data for the transactions.\n",
       "\n",
       "4. Write code to define the structure of the transaction and the operations that need to be performed.\n",
       "\n",
       "5. Create multiple threads or processes that will execute the transactions concurrently.\n",
       "\n",
       "6. Use synchronization mechanisms, such as locks or semaphores, to ensure that transactions do not interfere with each other.\n",
       "\n",
       "7. Test your implementation by running multiple transactions concurrently and verifying that they execute correctly and produce the expected results.\n",
       "\n",
       "By following these steps, you can implement concurrent transactions for the concurrency assignment in your database system."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_question = \"How do I implement concurrent transactions for the concurrency assignment\"\n",
    "pretty_print(query_gpt3(query_question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To implement concurrent transactions for the concurrency assignment, you will need to adhere to strict two-phase locking (2PL) where locks are acquired as needed and held until after committing changes to the database. This ensures no cascading rollbacks occur. Additionally, implement deadlock avoidance by detecting cycles in a \"waits-for\" graph to prevent deadlocks. Transactions should be managed using commands like transaction [begin|commit] to start or end a transaction, and lock <table> <key> to grab write locks on resources. Running the database as a server-client application allows for multiple clients, each handling one transaction at a time. Stress testing can be done using the bumble_stress executable with specified workloads to detect deadlocks or unsafe behavior."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Data retrieved from file: Assignment 6_ Concurrency.txt"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Data used to generate response: Transaction locks are completely unaware of the underlying representation of the data; we're only concerned in logical units to preserve the integrity of the external view of the database. On the other hand, data structure-level locks are completely unaware of the data it's locking; only the structure of how the data is stored. Thus, these two locking schemes are completely orthogonal to one another, and yet, are both essential for a database serving multiple tenants concurrently.\r\n",
       "\r\n",
       "\r\n",
       "2.6 Strict 2PL \r\n",
       "\r\n",
       "\r\n",
       "Our transactions will adhere to strict two-phase locking. That is, we will acquire locks as we need them, but we will hold on to all of them until after we have committed our changes to the database. One great corollary of this scheme is that we can be absolutely sure that there will not be cascading rollbacks; that is, if a transaction aborts, no other transaction will have to roll back because of it! This makes our lives a lot easier when implementing aborts, but does mean that our transactions may wait longer for resources to become accessible.\r\n",
       "\r\n",
       "\r\n",
       "2.7 Deadlock Avoidance\r\n",
       "\r\n",
       "\r\n",
       "We want to be sure that our transactions don't end up creating a deadlock. One way to do this is by detecting cycles in a \"waits-for\" graph. While a transaction is waiting for another transaction to free a particular resource, we should add an edge between it and the offending transaction to the \"waits-for\" graph. If a cycle is detected in this graph, that means that we have deadlocked, and will not be able to proceed without terminating one of the involved transactions. A reasonable policy here would be to terminate the last transaction that joined the graph. Remember to remove edges between transactions once transactions get terminated or are otherwise no longer waiting for a resource — otherwise, you may detect deadlocks that don't exist!\r\n",
       "\r\n",
       "\r\n",
       "3        Assignment Spec \r\n",
       "3.1 Overview\r\n",
       "\r\n",
       "\r\n",
       "In this project, you'll implement fine-grained locking on Hash and B+Tree, then resource-level locking, and finally implement deadlock detection and avoidance! Note that the assignment parts are somewhat isolated from each other; feel free to work out of order on this assignment.\r\n",
       "\r\n",
       "\r\n",
       "3.2 New REPL Commands\r\n",
       "\r\n",
       "\r\n",
       "The transaction REPL now supports two new commands:\r\n",
       "\r\n",
       "\r\n",
       "* transaction [begin|commit] - either starts or ends a transaction. Each client can have up to 1 transaction running at a time.\r\n",
       "* lock <table> <key> - grabs a write lock on a resource. Useful for debugging.\r\n",
       "\r\n",
       "\r\n",
       "3.3 Multiple Clients\r\n",
       "\r\n",
       "\r\n",
       "Since we need to deal with multiple clients, we need to run BumbleBase as a server-client application rather than just as a command-line application. Running ./bumble -p transaction should now start a server at port 8335. Then, run ./bumble_client -p 8335 to connect to the database and start running queries as normal! Using a tool like tmux might help you manage multiple terminals.\r\n",
       "\r\n",
       "\r\n",
       "In our implementation, each client can have up to one transaction running at a time. To simulate multiple transactions, we need multiple clients; hence, now, our database supports multiple clients through ./bumble_client. To begin a transaction, we run transaction begin, and to end it, transaction commit. Commands issued without an active transaction will be treated as a transaction of one action (transaction begin, [action], transaction commit).\r\n",
       "\r\n",
       "\r\n",
       "3.4 Stress Testing\r\n",
       "\r\n",
       "\r\n",
       "Because it can be useful to clobber your database to detect deadlocks or unsafe behavior using a shotgun approach, we've provided a new executable named bumble_stress to help with this. We've also provided a set of sample workloads in the workloads/ directory to run with bumble_stress - poke through workloads/README.md to get a sense of what each workload is doing, and feel free to make your own workloads!\r\n",
       "\r\n",
       "\r\n",
       "To stress test your database, build and run:\r\n",
       "\r\n",
       "\r\n",
       "./bumble_stress -index=<btree|hash> -workload=<w.txt> -n=<n> -verify. \r\n",
       "\t\r\n",
       "\r\n",
       "The workload file should essentially mimic what would normally be piped through STDIN, separated by newlines. The numthreads argument will specify the number of threads that will run the workload - to be clear, we split the workload across n threads, not duplicate the workload for n threads, meaning each line will only be run once, but on different threads. The index flag determines which kind of index you'll be stress testing. Lastly, the verify flag runs a verification check at the end of the stress test to ensure that the data structure is still consistent after the run. No project flag is required.\r\n",
       "\r\n",
       "\r\n",
       "Stress testing is an especially experimental feature in the course."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "full_query_engine.retriever.similarity_top_k = 1\n",
    "append_and_print(full_query_engine.query(query_question))\n",
    "print_data_sources(response_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To implement concurrent transactions for the concurrency assignment, it is crucial to ensure that your database supports fine-grained and resource-level locking. This involves implementing fine-grained locking on Hash Tables and B+Trees, as well as deadlock detection and avoidance mechanisms. Additionally, adhere to strict two-phase locking for transactions, acquiring locks as needed and holding them until changes are committed. Manage multiple clients by running BumbleBase as a server-client application, allowing each client to have up to one transaction running at a time. To simulate multiple transactions, multiple clients are needed, and transactions can be initiated with \"transaction begin\" and ended with \"transaction commit\" commands. Finally, stress testing using the provided executable \"bumble_stress\" and sample workloads can help detect deadlocks and ensure the consistency of your data structure after concurrent operations."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "full_query_engine.retriever.similarity_top_k = 10\n",
    "append_and_print(full_query_engine.query(query_question))\n",
    "#print_data_sources(response_list[-1], sources=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.core import ChatPromptTemplate\n",
    "\n",
    "qa_prompt_str = (\n",
    "    \"Context information is provided below. This context is meant to guide your understanding and not to be used for direct answers.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and based on your understanding, \"\n",
    "    \"please explain how you would approach solving the question: {query_str}\\n\"\n",
    "    \"Your response should demonstrate your reasoning process and how the concepts apply, rather than providing a direct solution.\"\n",
    ")\n",
    "# Text QA Prompt\n",
    "chat_text_qa_msgs = [\n",
    "    ChatMessage(\n",
    "        role=MessageRole.SYSTEM,\n",
    "        content=(\n",
    "            \"Please provide a summary and explanation, not direct excerpts or solutions to problems.\"\n",
    "        ),\n",
    "    ),\n",
    "    ChatMessage(role=MessageRole.USER, content=qa_prompt_str),\n",
    "]\n",
    "text_qa_template = ChatPromptTemplate(chat_text_qa_msgs)\n",
    "\n",
    "#Create a validator- a customized query engine\n",
    "index = load_index(FINAL_MILVUS_COLLECION_NAME)\n",
    "checker_engine= index.as_query_engine(text_qa_template=text_qa_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To write the `PopSelf()` function for the Go assignment, you need to understand the purpose of this function and how it should be implemented based on the provided context information.\n",
       "\n",
       "1. **Purpose of `PopSelf()` Function**:\n",
       "   - The `PopSelf()` function is used to remove a specific link from a list without the need to search through the entire list to find the link.\n",
       "   - The key idea is to handle the linking of the previous and next links of the given link to effectively remove it from the list.\n",
       "\n",
       "2. **Implementation Approach**:\n",
       "   - When implementing `PopSelf()`, you should focus on updating the `prev` and `next` links of the given link to effectively remove it from the list.\n",
       "   - You do not need to search through the list to find the link since the link is already provided.\n",
       "   - Ensure that after calling `PopSelf()` on a link, the list maintains its integrity and remains in a valid state.\n",
       "\n",
       "3. **Handling Errors**:\n",
       "   - If you encounter errors like \"invalid memory address or nil pointer dereference,\" it suggests that there might be an issue with how you are handling pointers or nil values in your implementation.\n",
       "   - Make sure to handle cases where pointers might be nil, especially when calling functions on them.\n",
       "\n",
       "4. **Guidance from Feedback**:\n",
       "   - Based on the feedback provided, ensure that your implementation of `PopSelf()` focuses on linking the previous and next links of the given link correctly to remove it from the list.\n",
       "   - Avoid calling functions on nil pointers, as this can lead to runtime errors.\n",
       "\n",
       "5. **Testing and Debugging**:\n",
       "   - Test your implementation thoroughly to ensure it works as expected in different scenarios.\n",
       "   - If you encounter errors, debug by checking how you are handling pointers, linking operations, and nil values in your code.\n",
       "\n",
       "By following these guidelines and understanding the purpose of the `PopSelf()` function, you can approach writing the function for the Go assignment effectively. Remember to focus on handling the linking operations correctly to remove the given link from the list without the need for additional searching."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_str= \"How do I write the popself function for the go assignment\"\n",
    "# Perform the query\n",
    "response = checker_engine.query(query_str)\n",
    "pretty_print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To perform the LeafNode split() function, you can utilize the leftPN and rightPN attributes within the Split class to store the newly created nodes. To access the new node after using split() in insert(), you may need to retrieve the corresponding page using node.page.GetPager().GetPage(node.rightSiblingPN) and then convert it to a leaf node using pageToLeafNode(newPage). However, it's important to note that setting the parent of the new node can be done within the split() function itself, and updating the parent only requires the page number, which can be obtained from the Split operation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Data retrieved from file: clean-ed-data.json"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Data retrieved from file: clean-ed-data.json"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Data retrieved from file: clean-ed-data.json"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Data retrieved from file: clean-ed-data.json"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Data retrieved from file: clean-ed-data.json"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Data retrieved from file: clean-ed-data.json"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Data retrieved from file: clean-ed-data.json"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Data retrieved from file: clean-ed-data.json"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Data retrieved from file: clean-ed-data.json"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Data retrieved from file: clean-ed-data.json"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "code_query = \"How do I do the LeafNode split() function\"\n",
    "append_and_print(full_query_engine.query(code_query))\n",
    "print_data_sources(response_list[-1], show_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_only = load_existing_index_as_query_engine(\"code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_code_prompt_str = \"\"\"\n",
    "Based on the provided information about CS1270, please answer the question to the best of your ability.\n",
    "---------------------\n",
    "{query_str}\n",
    "---------------------\n",
    "\"\"\"\n",
    "\n",
    "write_code_message = [\n",
    "   ChatMessage(role=MessageRole.SYSTEM, content=\"Please provide an answer to the question to the best of your ability, using the additional information if relevant\"),\n",
    "   ChatMessage(role=MessageRole.USER, content=write_code_prompt_str),\n",
    "]\n",
    "\n",
    "final_template = ChatPromptTemplate(write_code_message)\n",
    "code_only.update_prompts({'response_synthesizer:text_qa_template': final_template})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To provide guidance on how to implement the `split()` function for a `LeafNode` in the context of the given code snippet, you would need to understand the specific structure and requirements of the `LeafNode` class within the codebase.\n",
       "\n",
       "In general, a `split()` function for a `LeafNode` in a B-tree or similar data structure involves redistributing keys and values between two new leaf nodes when the original leaf node becomes full during an insertion operation.\n",
       "\n",
       "Here is a general outline of how you could implement a `split()` function for a `LeafNode` based on the provided code:\n",
       "\n",
       "1. Determine the midpoint index of the keys in the current `LeafNode`.\n",
       "2. Create a new `LeafNode` to store the keys and values that will be moved during the split.\n",
       "3. Copy the keys and values from the current `LeafNode` to the new `LeafNode`, starting from the midpoint index.\n",
       "4. Update the pointers of the neighboring leaf nodes to maintain the linked list structure.\n",
       "5. Insert the new key and value into the appropriate leaf node (either the current or the new one).\n",
       "6. Update the parent node with the new key if necessary.\n",
       "\n",
       "Remember, the specific implementation details may vary based on the exact requirements and structure of the `LeafNode` class in the provided code. It's essential to refer to the codebase, documentation, or any relevant examples to ensure that your implementation aligns with the expected behavior."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "append_and_print(code_only.query(\"How do I do the LeafNode split() function\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import IndexNode\n",
    "from llama_index.core import SummaryIndex\n",
    "\n",
    "base_retreiver = load_index(FINAL_MILVUS_COLLECION_NAME).as_retriever(similarity_top_k=10)\n",
    "code_retreiver = load_index(\"code\").as_retriever(similarity_top_k=10)\n",
    "\n",
    "base_obj = IndexNode(\n",
    "    index_id=\"vector\", obj=base_retreiver, text=\"Full information retriever\"\n",
    ")\n",
    "code_obj = IndexNode(\n",
    "    index_id=\"bm25\", obj=code_retreiver, text=\"Code only retriever\"\n",
    ")\n",
    "\n",
    "summary_index = SummaryIndex(objects=[base_obj, code_obj])\n",
    "composite_engine = summary_index.as_query_engine()\n",
    "composite_engine.update_prompts({'response_synthesizer:text_qa_template': final_template})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To implement the `split()` function for a `LeafNode` in Go, you can follow this basic example:\n",
       "\n",
       "```go\n",
       "type LeafNode struct {\n",
       "    keys  []int\n",
       "    value []string\n",
       "    next  *LeafNode\n",
       "}\n",
       "\n",
       "func (ln *LeafNode) split() (*LeafNode, int) {\n",
       "    mid := len(ln.keys) / 2\n",
       "    newKeys := make([]int, len(ln.keys)-mid)\n",
       "    newValues := make([]string, len(ln.value)-mid)\n",
       "\n",
       "    copy(newKeys, ln.keys[mid:])\n",
       "    copy(newValues, ln.value[mid:])\n",
       "\n",
       "    newLeaf := &LeafNode{\n",
       "        keys:  newKeys,\n",
       "        value: newValues,\n",
       "        next:  ln.next,\n",
       "    }\n",
       "\n",
       "    ln.keys = ln.keys[:mid]\n",
       "    ln.value = ln.value[:mid]\n",
       "    ln.next = newLeaf\n",
       "\n",
       "    return newLeaf, newKeys[0]\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_str= \"Look at the LeafNode split() function. It is not fully implemented. Write code in go to implement it using the information you have. Replace the panic with your implementation of the function\"\n",
    "append_and_print(composite_engine.query(query_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To implement the LeafNode split() function in Go, you can use the following code snippet:\n",
       "\n",
       "```go\n",
       "func (node *LeafNode) split() Split {\n",
       "    // Implement the split logic here\n",
       "    // You need to split the keys and values of the current node into two separate nodes\n",
       "    // Ensure that the split maintains the order of keys\n",
       "\n",
       "    // Example implementation:\n",
       "    left := &LeafNode{keys: node.keys[:len(node.keys)/2], values: node.values[:len(node.values)/2]}\n",
       "    right := &LeafNode{keys: node.keys[len(node.keys)/2:], values: node.values[len(node.values)/2:]}\n",
       "\n",
       "    node.keys = []int64{}\n",
       "    node.values = []int64{}\n",
       "\n",
       "    return Split{left: left, right: right, key: right.keys[0]}\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_str= \"Look at the LeafNode split() function. It is not fully implemented. Write code in go to implement it using the information you have. Replace the panic with your implementation of the function\"\n",
    "append_and_print(full_query_engine.query(query_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Data retrieved from file: clean-ed-data.json"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Data retrieved from file: clean-ed-data.json"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Data retrieved from file: clean-ed-data.json"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Data retrieved from file: clean-ed-data.json"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Data retrieved from file: clean-ed-data.json"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Data retrieved from file: clean-ed-data.json"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Data retrieved from file: clean-ed-data.json"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Data retrieved from file: Assignment 4_ Indexing.txt"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Data retrieved from file: clean-ed-data.json"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Data retrieved from file: node.go"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_data_sources(response_list[-1], show_text=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
